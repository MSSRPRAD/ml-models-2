# -*- coding: utf-8 -*-
"""Naive Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOAYYsWZqXhJ1BqQJtW-b1BvkGFjRApI
"""

import pandas as pd
import numpy as np

column_names = ["age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "salary"]

data = pd.read_csv("adult.data", names=column_names)
replace_values = {" <=50K":"1", " >=50K":"-1", " >50K":"-1", " <50K":"1"}
data.drop(['fnlwgt'], axis=1, inplace=True)

# data.drop(['fnlwgt'], axis=1, inplace=True)
# df_updated = df.replace(to_replace ='[nN]ew', value = 'New_', regex = True)
data['salary'] = data['salary'].replace(replace_values)
# strip whitespace before and after first and last character for all columns
data = data.applymap(lambda x: x.strip() if isinstance(x, str) else x)
data

# replace '?' with NaN
data = data.replace('?', np.nan)
data.isna().sum().sum()
data.dropna(inplace=True)

train = data.iloc[:20100, :]
test = data.iloc[20101:, :]
y = train['salary']
train = train.drop(['salary'], axis=1)

counts = y.groupby(y).count()
priors = counts.div(len(y))
priors[0]

priors

train1 = pd.concat([train, y], axis=1)

likelihood = {}
for col in train1.columns:
  likelihood[col] = train1.groupby(['salary', col]).size()
  likelihood[col]/=len(train1)
  likelihood[col]/=priors
  # likelihood[col].applymap(lambda x: x+1)
likelihood

x = test.iloc[0,:]

likelihood

x

# foo = likelihood['workclass']
# foo['1'][x[1]]

x

# j = '-1'
# count=0
# for i in x.index:
#   print(likelihood[i][j][x[count]])
#   count+=1
# j = '1'
# count=0
# for i in x.index:
#   print(likelihood[i][j][x[count]])
#   count+=1

class NaiveBayes:
  def __init__(self):
    self.priors = None
    self.cond_prob_of_feature = None
    self.class1 = None
    self.class2 = None
    pass

  def calc_prior_prob(self, y):
      self.priors = y.groupby(y).count().div(len(y))
      self.class1 = y.groupby(y).count()[0]
      self.class2 = y.groupby(y).count()[1]
      return  
    
  def calc_conditional_prob(self, X, y):
      self.cond_prob_of_feature = {}
      train = pd.concat([X,y], axis=1)
      for col in train.columns:
        self.cond_prob_of_feature[col] = train1.groupby(['salary', col]).size()
        self.cond_prob_of_feature[col]/=len(train1)
        self.cond_prob_of_feature[col]/=self.priors
      # self.cond_prob_of_feature['education'][('-1', 'Preschool')]=0.0
      return

  
  def fit(self, X, y):
    self.calc_prior_prob(y)
    self.calc_conditional_prob(X, y)
    return

  def classify(self, x):
    prob_class1 = 1
    prob_class2 = 1
    count=0
    print(x)
    for i in x.index:
      print("x[count]:"+str(x[count]))
      print("i:"+str(i))
      print('types:')
      print("x[count]:"+str(type(x[count])))
      print("i:"+str(type(i)))
      # if (i=='education' and x[count]=='Preschool') or (i=='education-num' and x[count]==1) or (i=='occupation' and (x[count]=='Priv-house-serv' or x[count]=='Armed-Forces')) or (i=='workclass' and x[count]=='Without-pay'):
      #   print('reached here')
      #   prob_class1 *= 0.0001
      #   prob_class2 *= self.cond_prob_of_feature[i]['1'][x[count]]
      # else:
    if(x[count] not in self.cond_prob_of_feature[i]['-1']):
        prob_class1 *= 0.0001  
    else:
        prob_class1 *= self.cond_prob_of_feature[i]['-1'][x[count]]
    if(x[count] not in self.cond_prob_of_feature[i]['1']):
        prob_class1 *= 0.0001  
    else:
        prob_class1 *= self.cond_prob_of_feature[i]['1'][x[count]]
    count+=1
   
    print(prob_class1)
    print(prob_class2)
    if prob_class1*self.priors[0] >= prob_class2*self.priors[1]:
      return -1 
    elif prob_class1*self.priors[0] > prob_class2*self.priors[1]:
      return 1
    else:
       return 1

  
  def predict(self, X):
    predictions = []
    for i in range(X.shape[0]):
      predictions.append(self.classify(X.iloc[i, :]))
    return predictions

  def accuracy(self, X, y):
    predictions = self.predict(self, X)
    misclassifications = 0
    for i in range(len(predictions)):
      if predictions.iloc[i] != y.iloc[i]:
        misclassifications += 1
    return misclassifications/len(predictions)

naive_bayes = NaiveBayes()
naive_bayes.fit(train, y)
naive_bayes.priors

naive_bayes.class1, naive_bayes.class2

test_y = test['salary']
test = test.drop(['salary'], axis=1)

test.iloc[0, :]

'Preschool' in naive_bayes.cond_prob_of_feature['education']['1']

predicted = naive_bayes.predict(test)

predicted

test_y

misclassifications=0
for i in range(len(predicted)):
  # print(str(predicted[i])+'|'+str(test_y.iloc[i]))
  # print(str(type(predicted[i]))+'|'+str(type(test_y.iloc[i])))
  # print(predicted[i]==test_y.iloc[i])
  if str(predicted[i])!=str(test_y.iloc[i]):
    misclassifications+=1

print(1.0-misclassifications/len(predicted))
